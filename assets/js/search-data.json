{
  
    
        "post0": {
            "title": "PyTorch 101",
            "content": "Table of Contents: . Packages | Matrices 2.1 Tensors from numpy arrays | 2.2 Tensors from scratch | 2.3 Numpy array from tensor | . | Tensor operations 3.1 Reshaping | 3.2 Indexing | . | Tensor arithmetic 4.1 Addition | 4.2 Subtraction | 4.3 Multipliciation | 4.4 Division | 4.5 Mean | 4.6 Standard Deviation | . | Gradients 5.1 Example using a single value | 5.2 Example using 1-D tensor | . | Linear Regression 6.1 Building a simple dataset | 6.2 Building the model | 6.3 Training the model | 6.4 Save the model | . | 1. Packages . back to top . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # pytorch import torch import torch.nn as nn # disable warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) . 2. Matrices . back to top . 2.1 Tensors from Numpy arrays . 2.1.1 Numpy arrays . arr = np.array([1,2,3,4,5,]) print(arr) print(arr.dtype) type(arr) . [1 2 3 4 5] int64 . numpy.ndarray . 2.1.2 Numpy arrays to Tensors . There are multiple ways to convert numpy array to a tensor. The different ways are: . torch.from_numpy() - This converts a numpy array to a tensor. | torch.as_tensor() - This is a general way to convert an object to tensor. | torch.tensor() - This creates a copy of the object as a tensor. | . x = torch.from_numpy(arr) print(x) print(x.dtype) print(type(x)) . tensor([1, 2, 3, 4, 5]) torch.int64 &lt;class &#39;torch.Tensor&#39;&gt; . x = torch.as_tensor(arr) print(x) print(x.dtype) print(type(x)) . tensor([1, 2, 3, 4, 5]) torch.int64 &lt;class &#39;torch.Tensor&#39;&gt; . For both of the above mentioned methods, there&#39;s a little caveat to understand. torch.from_numpy() and torch.as_tensor() creates a direct link between the numpy array and the tensor. Any change in the numpy array will affect the tensor. Following is an example. . arr = np.array([1,2,3,4,5,]) print(&quot;Numpy array : &quot;,arr) x = torch.from_numpy(arr) print(&quot;PyTorch tensor : &quot;,x) # chaning the 0th index of arr arr[0] = 999 print(&quot;Numpy array : &quot;,arr) print(&quot;PyTorch tensor : &quot;,x) . Numpy array : [1 2 3 4 5] PyTorch tensor : tensor([1, 2, 3, 4, 5]) Numpy array : [999 2 3 4 5] PyTorch tensor : tensor([999, 2, 3, 4, 5]) . It shows that though torch.from_numpy() converts the array to tensor, but it creates a reference to the same object. To create a copy of the array, torch.tensor() can be used. . arr = np.array([1,2,3,4,5,]) print(&quot;Numpy array : &quot;,arr) x = torch.tensor(arr) print(&quot;PyTorch tensor : &quot;,x) # chaning the 0th index of arr arr[0] = 999 print(&quot;Numpy array : &quot;,arr) print(&quot;PyTorch tensor : &quot;,x) . Numpy array : [1 2 3 4 5] PyTorch tensor : tensor([1, 2, 3, 4, 5]) Numpy array : [999 2 3 4 5] PyTorch tensor : tensor([1, 2, 3, 4, 5]) . 2.2 Tensors from scratch . Similar to numpy functions such as ones, eye &amp; arange etc, torch provides some handy functions to work with. . 2.2.1 Allocate space in memory for tensor . torch.empty(4,2) . tensor([[0.0000e+00, 0.0000e+00], [1.8788e+31, 1.7220e+22], [6.0434e-07, 5.3175e-08], [1.0244e-11, 1.0548e-08]]) . 2.2.2 Tensor of Zeros . torch.zeros(4,3) . tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) . 2.2.3 Tensor of Ones . torch.ones(3,4) . tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . 2.2.4 Evenly spaced values within a given interval . torch.arange(0,50,10) . tensor([ 0, 10, 20, 30, 40]) . 2.2.5 Evenly spaced numbers over a specified interval . torch.linspace(0,50,10) . tensor([ 0.0000, 5.5556, 11.1111, 16.6667, 22.2222, 27.7778, 33.3333, 38.8889, 44.4444, 50.0000]) . 2.2.6 Python list to tensor . my_tensor = torch.tensor([1,2,3]) print(my_tensor) print(my_tensor.dtype) type(my_tensor) . tensor([1, 2, 3]) torch.int64 . torch.Tensor . 2.2.7 Random number from uniform distribution . torch.rand(4,3) . tensor([[0.9425, 0.1938, 0.9745], [0.0672, 0.4665, 0.3066], [0.2388, 0.9712, 0.2791], [0.5487, 0.0524, 0.1219]]) . 2.2.8 Random number from standard normal uniform distribution - mean 0 and standard deviation 1 . torch.randn(4,3) . tensor([[ 0.1930, -0.4929, 0.2210], [-1.0913, 0.1019, 0.6927], [-2.2604, 0.1454, -0.0999], [ 0.1105, -0.6836, -1.1310]]) . 2.2.9 Random integers . torch.randint(low=0,high=10,size=(5,5)) . tensor([[3, 2, 4, 2, 1], [8, 4, 8, 3, 5], [5, 4, 0, 1, 4], [1, 6, 0, 5, 5], [5, 1, 6, 0, 6]]) . 2.2.10 Random tensor with same shape as incoming tensor . x = torch.zeros(2,5) print(&quot;Shape of incoming tensor&quot;) print(x.shape) print(&quot;-&quot;*50) print(&quot;Random tensor with uniform distribution&quot;) print(torch.rand_like(x)) print(&quot;-&quot;*50) print(&quot;Random tensor with standard normal uniform distribution&quot;) print(torch.randn_like(x)) print(&quot;-&quot;*50) print(&quot;Random tensor with integers&quot;) print(torch.randint_like(x,low=0,high=10)) . Shape of incoming tensor torch.Size([2, 5]) -- Random tensor with uniform distribution tensor([[0.1401, 0.7472, 0.5479, 0.6027, 0.1381], [0.3237, 0.5044, 0.4604, 0.4910, 0.9068]]) -- Random tensor with standard normal uniform distribution tensor([[ 0.1214, 1.3723, 0.8735, 2.9361, 0.0329], [-1.6780, -1.2402, -0.1299, 1.0579, 0.1606]]) -- Random tensor with integers tensor([[2., 8., 1., 6., 9.], [2., 3., 5., 9., 7.]]) . 2.3 Numpy array from tensor . print(&quot;The tensor&quot;) print(x) arr = x.numpy() print(&quot;-&quot;*50) print(&quot;The numpy array&quot;) print(arr) print(arr.dtype) print(type(arr)) . The tensor tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) -- The numpy array [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] float32 &lt;class &#39;numpy.ndarray&#39;&gt; . 3. Tensor operations . back to top . 3.1 Reshaping . 3.1.1 Using reshape() . x = torch.arange(10) print(&quot;Reshaped tensor&quot;) print(x.reshape(2,5)) print(&quot;-&quot;*50) print(&quot;Original tensor&quot;) print(x) . Reshaped tensor tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) -- Original tensor tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . 3.1.2 Using view() . x = torch.arange(10) print(&quot;Reshaped tensor&quot;) print(x.view(2,5)) print(&quot;-&quot;*50) print(&quot;Original tensor&quot;) print(x) . Reshaped tensor tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) -- Original tensor tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . 3.1.3 Shape inference . x = torch.arange(10) print(x.shape) print(&quot;-&quot;*50) z = x.view(2,-1) print(z) print(z.shape) . torch.Size([10]) -- tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) torch.Size([2, 5]) . As in the above case, if we put -1 as the second dimension, it will itelf infer the same provided it makes sense. Try changing the above code to get a shape of (3,-1) which will provide an error; since a tensor of size 10 can&#39;t be reshaped to a tensor with first dimension as 3. . 3.2 Indexing . 3.2.1 Indexing . x = torch.arange(6).reshape(3,2) print(x) print(x[1,1]) type(x[1,1]) . tensor([[0, 1], [2, 3], [4, 5]]) tensor(3) . torch.Tensor . 3.2.2 Selecting . x = torch.arange(6).reshape(3,2) print(x) print(&quot;_&quot;*50) print(x[:,1]) print(x[:,1].shape) print(&quot;_&quot;*50) print(x[:,1:]) print(x[:,1:].shape) . tensor([[0, 1], [2, 3], [4, 5]]) __________________________________________________ tensor([1, 3, 5]) torch.Size([3]) __________________________________________________ tensor([[1], [3], [5]]) torch.Size([3, 1]) . 4. Tensor arithmetic . back to top . There are multiple ways to do arithmetic. Direct operators as well as torch functions can be used. . 4.1 Addition . 4.1.1 Element-wise addition . a = torch.tensor([1.,2.,3.]) b = torch.tensor([4.,5.,6.]) print(a+b) print(torch.add(a,b)) . tensor([5., 7., 9.]) tensor([5., 7., 9.]) . 4.1.2 In-place addition . a = torch.tensor([1.,2.,3.]) b = torch.tensor([4.,5.,6.]) a.add_(b) print(a) . tensor([5., 7., 9.]) . The in-place arithmetic is applicable for all operations - add_, sub_, mul_ etc. . 4.2 Subtraction . a = torch.tensor([1,2,3,4]) b = torch.tensor([1,1,1,1]) print(a-b) print(torch.sub(a,b)) . tensor([0, 1, 2, 3]) tensor([0, 1, 2, 3]) . 4.3 Multiplication . a = torch.ones(2,2) b = torch.zeros(2,2) print(a*b) print(torch.mul(a,b)) . tensor([[0., 0.], [0., 0.]]) tensor([[0., 0.], [0., 0.]]) . 4.4 Division . a = torch.ones(2,2) b = torch.zeros(2,2) print(a/b) print(torch.div(a,b)) . tensor([[inf, inf], [inf, inf]]) tensor([[inf, inf], [inf, inf]]) . The tensor contains inf since any number divided by 0 is infinity. . 4.4 Mean . a = torch.arange(1,11,1,dtype=torch.float64) print(a.mean(dim=0)) . tensor(5.5000, dtype=torch.float64) . a = torch.arange(1,11,1).mean(dim=0) won&#39;t work since this would create a tensor of int and the mean can only be calculated for float in PyTorch. . 4.5 Standard Deviation . a = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) print(a.std(dim=0)) . tensor(3.0277) . 5. Gradients . back to top . A gradient, also called slope is simply defined as the rate of change of the functon at a particular point. PyTorch can easily calculate gradients and accumulate them. Just a single parameter requires_grad is used while defining tensors to track the gradient. . 5.1 Example using a single value . 5.1.1 Defining a 1-D tensor x with a single value . x = torch.tensor([2],dtype=torch.float64, requires_grad = True) x . tensor([2.], dtype=torch.float64, requires_grad=True) . 5.1.2 Calculating a function y depending upon x . $$ y = 5(x+1)^2 $$ y = 5 * ((x + 1)**2) y . tensor([45.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;) . 5.1.3 Calculating the gradient of y wrt x . $$ dy/dx = 10(x+1) $$$$ dy/dx = 10(2+1) = 30 $$ y.backward() x.grad . tensor([30.], dtype=torch.float64) . 5.2 Example using 1-D tensor . 5.2.1 Defining a 1-D tensor x with more than 1 value . x = torch.ones(2,requires_grad = True) x . tensor([1., 1.], requires_grad=True) . 5.2.2 Calculating a function y depending upon x . $$ y = 5(x+1)^2 $$ y = 5 * ((x + 1)**2) y . tensor([20., 20.], grad_fn=&lt;MulBackward0&gt;) . We can observe that y is also a tensor with more than one value. But backward can only be called on a scaler or a 1-element tensor. We can create another function let&#39;s say o depending upon y. . $$ o = 1/2 * sum_{n=x_i}^{x_n} y_i $$ o = 0.5 * torch.sum(y) o . tensor(20., grad_fn=&lt;MulBackward0&gt;) . 5.2.3 Calculating the gradient of o wrt x . $$ o = 1/2 * sum_{n=x_i}^{x_n} 5(x_i+1)^2 $$$$ do/dx_i = 1/2 * 10(x_i+1) $$$$ do/dx_1 = 1/2 * 10(1+1) = 10 $$ o.backward() x.grad . tensor([10., 10.]) . 6. Linear Regression . back to top . Linear Regression allows to understand the relationship between 2 continuous variables. For example x is an independent variable and y is dependent upon x, then the linear equation would be as follows with a as the slope of the equation and b as the intercept. $$ y = ax+b $$ In this part of notebook, I&#39;m creating a Linear Regression model in PyTorch. . 6.1 Building a simple dataset . back to top . 6.1.1 Defining values . x = np.arange(0,11,1,dtype=np.float32) # creating an independent variable x y = 2 * x + 1 # creating a dependent variable y print(x.shape) print(y.shape) print(x) print(y) . (11,) (11,) [ 0. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10.] [ 1. 3. 5. 7. 9. 11. 13. 15. 17. 19. 21.] . 6.1.2 Reshaping arrays for PyTorch model . x=x.reshape(-1,1) y=y.reshape(-1,1) print(x.shape, y.shape) . (11, 1) (11, 1) . 6.1.3 Plotting the linear curve . This is the plot of the equation that states y=2x+1. . plt.scatter(x,y) plt.plot(x,y,color=&quot;Orange&quot;,linewidth=2) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.title(&quot;Y = 2X + 1&quot;) plt.show() . 6.2 Building the model . back to top . 6.2.1 Model Class . class LinearRegressionModel(nn.Module): def __init__(self,input_dim, output_dim): super(LinearRegressionModel,self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self,x): out = self.linear(x) return out . 6.2.2 Instantiate the model . input_dim=1 output_dim=1 model = LinearRegressionModel(input_dim,output_dim) . 6.2.3 Instantiate optimizer and loss . Many loss functions can be used according to the problem statement. Linear regression mostly uses MSE or RMSE. I&#39;ll be using MSE for this case i.e Mean square error and its formula is as follows. $$ MSE = 1/n sum_{n=1}^{n} (y_p - y_i)^2 $$ . criterion=nn.MSELoss() # loss function optimizer = torch.optim.SGD(model.parameters(), lr=0.01) . 6.3 Training the model . back to top . 6.3.1 The training loop . epochs = 100 for epoch in range(epochs): # convert numpy arrays to tensors inputs=torch.tensor(x,dtype=torch.float32, requires_grad=True) labels=torch.tensor(y,dtype=torch.float32) # clear the gradients optimizer.zero_grad() # forward to get output outputs=model(inputs) # calculating loss loss=criterion(outputs,labels) # backpropagation loss.backward() # updating parameters optimizer.step() print(&quot;Training complete...&quot;) . Training complete... . 6.3.2 Predictions . The true values are . y . array([[ 1.], [ 3.], [ 5.], [ 7.], [ 9.], [11.], [13.], [15.], [17.], [19.], [21.]], dtype=float32) . The predicted values are . predicted = model(torch.tensor(x,requires_grad=True)).data.numpy() predicted . array([[ 0.13100429], [ 2.2561476 ], [ 4.381291 ], [ 6.5064344 ], [ 8.6315775 ], [10.756721 ], [12.881865 ], [15.007008 ], [17.13215 ], [19.257294 ], [21.382437 ]], dtype=float32) . 6.3.3 Plot predictions and true value . plt.plot(x, y, &#39;go&#39;, color=&quot;Blue&quot;,label=&#39;True data&#39;,) # Plot predictions plt.plot(x, predicted, &#39;--&#39;,color=&quot;Red&quot;, label=&#39;Predictions&#39;) # Legend and plot plt.legend(loc=&#39;best&#39;) plt.show() . 6.3 Save the model . back to top . After this step, you may check out ./linear_regression_model.pkl file path, where the model would be saved. . torch.save(model.state_dict(),&#39;linear_regression_model.pkl&#39;) . I&#39;ll be updating it regularly. If it helped you, consider giving an upvote. &#9996;&#127996; . back to top . Check my other notebooks . https://www.kaggle.com/namanmanchanda/asl-detection-99-accuracy | https://www.kaggle.com/namanmanchanda/rnn-in-pytorch | https://www.kaggle.com/namanmanchanda/heart-attack-eda-prediction-90-accuracy | https://www.kaggle.com/namanmanchanda/stroke-eda-and-ann-prediction | The code in this notebook has been referenced from various sources available online, some of which are . https://www.deeplearningwizard.com | https://www.udemy.com/course/pytorch-for-deep-learning-with-python-bootcamp/ |",
            "url": "https://namanmanchanda09.github.io/mindAI/jupyter/pytorch/python/programming/2021/05/29/pytorch-101.html",
            "relUrl": "/jupyter/pytorch/python/programming/2021/05/29/pytorch-101.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Cat vs Dog Classifier",
            "content": "Cat vs Dog Classifier . fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. fastai applications include: . Vision | Text | Tabular | Collaborative filtering | . 1. Packages . from fastai.vision.all import * # line 1 . 2. Data . untar_data(url, fname=None, dest=None, c_key=&#39;data&#39;, force_download=False, extract_func=file_extract, timeout=4) . untar_data is a very powerful convenience function to download files from url to dest. The url can be a default url from the URLs class or a custom url. If dest is not passed, files are downloaded at the default_dest which defaults to ~/.fastai/data/. This convenience function extracts the downloaded files to dest by default. In order, to simply download the files without extracting, pass the noop function as extract_func. . This line downloads a standard dataset from the fast.ai datasets collection (if not previously downloaded) to your server, extracts it (if not previously extracted), and returns a Path object with the extracted location. . path = untar_data(URLs.PETS)/&#39;images&#39; # line 2 . The Pet dataset contains 7,390 pictures of dogs and cats, consisting of 37 breeds. . 3. DataLoader . 3.1 Label function . The names of the images starting with an uppercase letter are cat images and the rest are the dog images. . def is_cat(x): return x[0].isupper() # line 3 . 3.2 Image Data Loader . ImageDataLoaders(loaders, path=&#39;.&#39;, device=None) . This is a wrapper around severalDataLoaders with factory methods for computer vision problems. . This class should not be used directly, one of the factory methods should be preferred instead. All those factory methods accept as arguments:- item_tfms: one or several transforms applied to the items before batching them- batch_tfms: one or several transforms applied to the batches once they are formed . bs: the batch size | val_bs: the batch size for the validation DataLoader (defaults to bs) | shuffle_train: if we shuffle the training DataLoader or not | device: the PyTorch device to use (defaults to default_device()) | . dls = ImageDataLoaders.from_name_func( # line 4 path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) . The first part of the class name will generally be the type of data you have, such as image or text. . The other important piece of information that we have to tell fastai is how to get the labels from the dataset. Computer vision datasets are normally structured in such a way that the label for an image is part of the filename or path—most commonly the parent folder name. Here we’re telling fastai to use the is_cat function we just defined. . Finally, we define the Transforms that we need. A Transform contains code that is applied automatically during training; fastai includes many predefined Transforms, and adding new ones is as simple as creating a Python function. . There are two kinds: item_tfms are applied to each item (in this case, each item is resized to a 224-pixel square), while batch_tfms are applied to a batch of items at a time using the GPU, so they’re particularly fast. . The most important parameter to mention here is valid_pct=0.2. This tells fastai to hold out 20% of the data and not use it for training the model at all. . 4. Modelling . cnn_learner(dls, arch, normalize=True, n_out=None, pretrained=True, config=None, loss_func=None, opt_func=Adam, lr=0.001, splitter=None, cbs=None, metrics=None, path=None, model_dir=&#39;models&#39;...) . Build a convnet style learner from dls and arch . The model is built from arch using the number of final activations inferred from dls if possible (otherwise pass a value to n_out). It might be pretrained and the architecture is cut and split using the default metadata of the model architecture (this can be customized by passing a cut or a splitter). . learn = cnn_learner(dls, resnet34, metrics=error_rate) # line 5 learn.fine_tune(1) # line 6 . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . epoch train_loss valid_loss error_rate time . 0 | 0.160443 | 0.025996 | 0.008119 | 00:45 | . epoch train_loss valid_loss error_rate time . 0 | 0.072577 | 0.027073 | 0.008119 | 00:44 | . The sixth line of our code tells fastai how to fit the model. The architecture only describes a template for a mathematical function; it doesn’t actually do anything until we provide values for the millions of parameters it contains. . 5. Prediction . 5.1 Widget module for uploading image . Not a part of model building process . import ipywidgets as widgets # line 7 uploader = widgets.FileUpload() # line 8 uploader . 5.2 Prediction function - helper() . def helper(): # line 9 # plotting the image img = PILImage.create(uploader.data[0]) img.show() # predicting the image is_cat,_,probs = learn.predict(img) print(f&quot;Is this a cat?: {is_cat}.&quot;) print(f&quot;Probability it&#39;s a cat: {probs[1].item():.6f}&quot;) . 5.3 Predicting the image . helper() # line 10 . Is this a cat?: True. Probability it&#39;s a cat: 1.000000 . helper() . Is this a cat?: False. Probability it&#39;s a cat: 0.000003 . It is quite evident from prediction probabilities that fastai can be used to develop state of the art ML models in just a few lines of code. . All the code in the notebook is the boilerplate code available on the fast.ai website. . The documentation in the notebook is also taken from the official fastai docs that can be found here. . If you liked the notebook, please leave an upvote. Thank you ;) . back to top . Check out my other notebooks:- . https://www.kaggle.com/namanmanchanda/star-wars-classifier | https://www.kaggle.com/namanmanchanda/pima-indian-diabetes-eda-and-prediction | https://www.kaggle.com/namanmanchanda/tps-april-complete-eda-prediction |",
            "url": "https://namanmanchanda09.github.io/mindAI/fastai/jupyter/python/pytorch/2021/05/29/cat-vs-dog-classifier-10-lines-of-code-fast-ai.html",
            "relUrl": "/fastai/jupyter/python/pytorch/2021/05/29/cat-vs-dog-classifier-10-lines-of-code-fast-ai.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Star Wars Classifier",
            "content": "Star Wars Classifier . . This notebook consists of building a Star Wars classifier from scratch. The notebook doesn&#39;t use any predefined dataset. So, I&#39;ll be downloading the dataset on the go by scraping the images from internet. For the sake of keeping it simple, I&#39;ll be making a 3 class classifier mainly of Yoda, Luke and Wookie. The model development will be done using fastai. If you like the notebook, consider giving an upvote. ✅ Table of Contents: . Downloads | Packages | Pre-model building 3.1 Create folder | 3.2 Scrape images | 3.3 Move images | . | Data Loaders 4.1 For a single label | 4.2 For the model building | . | Model Building 5.1 Training | 5.2 Prediction | . | 1. Downloads . back to top . I&#39;m using a python package named icrawler for scraping the images. . !pip install icrawler . Collecting icrawler Downloading icrawler-0.6.4.tar.gz (26 kB) Collecting beautifulsoup4&gt;=4.4.1 Using cached beautifulsoup4-4.9.3-py3-none-any.whl (115 kB) Collecting lxml Using cached lxml-4.6.3-cp38-cp38-macosx_10_9_x86_64.whl (4.6 MB) Collecting requests&gt;=2.9.1 Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 5.1 MB/s eta 0:00:01 Requirement already satisfied: six&gt;=1.10.0 in /Users/namanmanchanda/miniconda3/envs/nishu/lib/python3.8/site-packages (from icrawler) (1.15.0) Requirement already satisfied: Pillow in /Users/namanmanchanda/miniconda3/envs/nishu/lib/python3.8/site-packages (from icrawler) (8.1.1) Collecting soupsieve&gt;1.2 Using cached soupsieve-2.2.1-py3-none-any.whl (33 kB) Collecting idna&lt;3,&gt;=2.5 Downloading idna-2.10-py2.py3-none-any.whl (58 kB) |████████████████████████████████| 58 kB 9.3 MB/s eta 0:00:01 Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/namanmanchanda/miniconda3/envs/nishu/lib/python3.8/site-packages (from requests&gt;=2.9.1-&gt;icrawler) (2020.12.5) Collecting chardet&lt;5,&gt;=3.0.2 Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB) |████████████████████████████████| 178 kB 19.4 MB/s eta 0:00:01 Collecting urllib3&lt;1.27,&gt;=1.21.1 Downloading urllib3-1.26.5-py2.py3-none-any.whl (138 kB) |████████████████████████████████| 138 kB 9.4 MB/s eta 0:00:01 Building wheels for collected packages: icrawler Building wheel for icrawler (setup.py) ... done Created wheel for icrawler: filename=icrawler-0.6.4-py2.py3-none-any.whl size=35063 sha256=bd50c3f1d07da534fa4b7615bd47053e2d92da940734f7757b3c9d172fc18b9d Stored in directory: /Users/namanmanchanda/Library/Caches/pip/wheels/5b/a5/50/db28e1726fdc127cb6c5a757a4350af44a32b4e5d2c5d45dac Successfully built icrawler Installing collected packages: urllib3, soupsieve, idna, chardet, requests, lxml, beautifulsoup4, icrawler Successfully installed beautifulsoup4-4.9.3 chardet-4.0.0 icrawler-0.6.4 idna-2.10 lxml-4.6.3 requests-2.25.1 soupsieve-2.2.1 urllib3-1.26.5 . . 2. Packages . back to top . import os import numpy as np import pandas as pd import matplotlib.pyplot as plt for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # icrawler from icrawler.builtin import GoogleImageCrawler # fastai from fastai import * from fastai.vision import * from fastai.imports import * from fastai.vision.all import * # widgets import ipywidgets as widgets # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) . 3. Pre-model building . back to top . 3.1 Create folder . I&#39;m creating three folders in /kaggle/working to download their respective image in each folder. . !mkdir yoda !mkdir luke !mkdir wookie . 3.2 Scrape images . The way icrawler works is that it creates a folder named /images from wherever the command is run. So right now we are in the /kaggle/working directory. Now I&#39;ll be going to each directory one at a time and run the crawler to download the images in the /images folder. So the structure of the folders will be something like . /kaggle/working/yoda/images | /kaggle/working/luke/images | /kaggle/working/wookie/images | . After the download, I&#39;ll be moving the images from the images folder of each respective label to the label folder itself - so for example from /kaggle/working/yoda/images to /kaggle/working/yoda and I&#39;ll be deleting all the empty images folder. . If you would like to reproduce the exact same thing, run the command in console first followed by the command in the notebook and so on in the provided order which is as follows. . Run the following in console . cd yoda After above command, run the following cell . google_crawler = GoogleImageCrawler() google_crawler.crawl(keyword=&#39;baby yoda&#39;, max_num=50) . Run the following in console one line at a time . cd .. cd luke After above command, run the following cell . google_crawler = GoogleImageCrawler() google_crawler.crawl(keyword=&#39;luke skywalker&#39;, max_num=50) . Run the following in console one line at a time . cd .. cd wookie After above command, run the following cell . google_crawler = GoogleImageCrawler() google_crawler.crawl(keyword=&#39;wookie&#39;, max_num=50) . 3.3 Move images . Run the following in console one line at a time . cd .. Now I&#39;ll be moving the images from /images folders to their respective labels and deleting the /images folder. Run the following in console one line at a time . mv -v yoda/images/* yoda mv -v luke/images/* luke mv -v wookie/images/* wookie rmdir yoda/images rmdir luke/images rmdir wookie/images Once done, you may check run pwd in console to check your current working directory. It must show /kaggle/working. . 4. Data Loaders . back to top . 4.1 For a single label . path = Path(&#39;/kaggle/working/yoda&#39;) dls = ImageDataLoaders.from_folder(path, valid_pct=0.5, batch_size=10, item_tfms=Resize(224)) dls.valid.show_batch(max_n=4, nrows=1) . 4.2 For the model building . characters = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(256)) # Creating the dataloader path = Path(&#39;/kaggle/working&#39;) dls = characters.dataloaders(path) # checking the images dls.valid.show_batch(max_n=18, nrows=3) . 5. Model Building . back to top . 5.1 Training . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . epoch train_loss valid_loss error_rate time . 0 | 1.834176 | 2.258008 | 0.600000 | 00:07 | . epoch train_loss valid_loss error_rate time . 0 | 1.978312 | 1.446693 | 0.566667 | 00:06 | . 1 | 1.585535 | 0.461595 | 0.133333 | 00:05 | . 2 | 1.268723 | 0.311160 | 0.066667 | 00:05 | . 3 | 1.044068 | 0.287985 | 0.066667 | 00:05 | . 5.2 Prediction . uploader = widgets.FileUpload() uploader . def helper(): img = PILImage.create(uploader.data[0]) img.show() pred,pred_idx,probs = learn.predict(img) lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; print(lbl_pred) . helper() . Label(value=&#39;Prediction: yoda; Probability: 0.9999&#39;) . helper() . Label(value=&#39;Prediction: luke; Probability: 0.9567&#39;) . helper() . Label(value=&#39;Prediction: wookie; Probability: 0.9970&#39;) . If you liked the notebook, please drop a upvote. Thank you.✅ Check out my other notebooks here: . https://www.kaggle.com/namanmanchanda/cat-vs-dog-classifier-10-lines-of-code-fast-ai | https://www.kaggle.com/namanmanchanda/titanic-eda | https://www.kaggle.com/namanmanchanda/pima-indian-diabetes-eda-and-prediction |",
            "url": "https://namanmanchanda09.github.io/mindAI/fastpages/jupyter/2021/05/27/starwars.html",
            "relUrl": "/fastpages/jupyter/2021/05/27/starwars.html",
            "date": " • May 27, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Naman and I’m in my senior year of study pursuing engineering from GGSIPU. What drives me is DATA. I love to play with Data and the most fun part, the story it tells. . I believe in continous learning and so, keep exploring every technical field. I have explored every field from Blockchain to Deep Learning. I have created a few decentralised applications on Ethereum, full stack apps and done data analysis. I have created some really cool projects using Computer Vision technology some of which include American Sign Language Detection etc. Check out my links to know more about me. . Blog | Twitter | Github | Linkedin | Kaggle | Anything urgent, text me on Discord - Naman Manchanda#8016 🙂 .",
          "url": "https://namanmanchanda09.github.io/mindAI/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://namanmanchanda09.github.io/mindAI/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}