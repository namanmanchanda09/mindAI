{
  
    
        "post0": {
            "title": "Cat vs Dog Classifier",
            "content": "Cat vs Dog Classifier . fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. fastai applications include: . Vision | Text | Tabular | Collaborative filtering | . 1. Packages . from fastai.vision.all import * # line 1 . 2. Data . untar_data(url, fname=None, dest=None, c_key=&#39;data&#39;, force_download=False, extract_func=file_extract, timeout=4) . untar_data is a very powerful convenience function to download files from url to dest. The url can be a default url from the URLs class or a custom url. If dest is not passed, files are downloaded at the default_dest which defaults to ~/.fastai/data/. This convenience function extracts the downloaded files to dest by default. In order, to simply download the files without extracting, pass the noop function as extract_func. . This line downloads a standard dataset from the fast.ai datasets collection (if not previously downloaded) to your server, extracts it (if not previously extracted), and returns a Path object with the extracted location. . path = untar_data(URLs.PETS)/&#39;images&#39; # line 2 . The Pet dataset contains 7,390 pictures of dogs and cats, consisting of 37 breeds. . 3. DataLoader . 3.1 Label function . The names of the images starting with an uppercase letter are cat images and the rest are the dog images. . def is_cat(x): return x[0].isupper() # line 3 . 3.2 Image Data Loader . ImageDataLoaders(loaders, path=&#39;.&#39;, device=None) . This is a wrapper around severalDataLoaders with factory methods for computer vision problems. . This class should not be used directly, one of the factory methods should be preferred instead. All those factory methods accept as arguments:- item_tfms: one or several transforms applied to the items before batching them- batch_tfms: one or several transforms applied to the batches once they are formed . bs: the batch size | val_bs: the batch size for the validation DataLoader (defaults to bs) | shuffle_train: if we shuffle the training DataLoader or not | device: the PyTorch device to use (defaults to default_device()) | . dls = ImageDataLoaders.from_name_func( # line 4 path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) . The first part of the class name will generally be the type of data you have, such as image or text. . The other important piece of information that we have to tell fastai is how to get the labels from the dataset. Computer vision datasets are normally structured in such a way that the label for an image is part of the filename or path—most commonly the parent folder name. Here we’re telling fastai to use the is_cat function we just defined. . Finally, we define the Transforms that we need. A Transform contains code that is applied automatically during training; fastai includes many predefined Transforms, and adding new ones is as simple as creating a Python function. . There are two kinds: item_tfms are applied to each item (in this case, each item is resized to a 224-pixel square), while batch_tfms are applied to a batch of items at a time using the GPU, so they’re particularly fast. . The most important parameter to mention here is valid_pct=0.2. This tells fastai to hold out 20% of the data and not use it for training the model at all. . 4. Modelling . cnn_learner(dls, arch, normalize=True, n_out=None, pretrained=True, config=None, loss_func=None, opt_func=Adam, lr=0.001, splitter=None, cbs=None, metrics=None, path=None, model_dir=&#39;models&#39;...) . Build a convnet style learner from dls and arch . The model is built from arch using the number of final activations inferred from dls if possible (otherwise pass a value to n_out). It might be pretrained and the architecture is cut and split using the default metadata of the model architecture (this can be customized by passing a cut or a splitter). . learn = cnn_learner(dls, resnet34, metrics=error_rate) # line 5 learn.fine_tune(1) # line 6 . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . epoch train_loss valid_loss error_rate time . 0 | 0.160443 | 0.025996 | 0.008119 | 00:45 | . epoch train_loss valid_loss error_rate time . 0 | 0.072577 | 0.027073 | 0.008119 | 00:44 | . The sixth line of our code tells fastai how to fit the model. The architecture only describes a template for a mathematical function; it doesn’t actually do anything until we provide values for the millions of parameters it contains. . 5. Prediction . 5.1 Widget module for uploading image . Not a part of model building process . import ipywidgets as widgets # line 7 uploader = widgets.FileUpload() # line 8 uploader . 5.2 Prediction function - helper() . def helper(): # line 9 # plotting the image img = PILImage.create(uploader.data[0]) img.show() # predicting the image is_cat,_,probs = learn.predict(img) print(f&quot;Is this a cat?: {is_cat}.&quot;) print(f&quot;Probability it&#39;s a cat: {probs[1].item():.6f}&quot;) . 5.3 Predicting the image . helper() # line 10 . Is this a cat?: True. Probability it&#39;s a cat: 1.000000 . helper() . Is this a cat?: False. Probability it&#39;s a cat: 0.000003 . It is quite evident from prediction probabilities that fastai can be used to develop state of the art ML models in just a few lines of code. . All the code in the notebook is the boilerplate code available on the fast.ai website. . The documentation in the notebook is also taken from the official fastai docs that can be found here. . If you liked the notebook, please leave an upvote. Thank you ;) . back to top . Check out my other notebooks:- . https://www.kaggle.com/namanmanchanda/star-wars-classifier | https://www.kaggle.com/namanmanchanda/pima-indian-diabetes-eda-and-prediction | https://www.kaggle.com/namanmanchanda/tps-april-complete-eda-prediction |",
            "url": "https://namanmanchanda09.github.io/mindAI/fastai/jupyter/python/pytorch/2021/05/29/cat-vs-dog-classifier-10-lines-of-code-fast-ai.html",
            "relUrl": "/fastai/jupyter/python/pytorch/2021/05/29/cat-vs-dog-classifier-10-lines-of-code-fast-ai.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Star Wars Classifier",
            "content": "Star Wars Classifier . . This notebook consists of building a Star Wars classifier from scratch. The notebook doesn&#39;t use any predefined dataset. So, I&#39;ll be downloading the dataset on the go by scraping the images from internet. For the sake of keeping it simple, I&#39;ll be making a 3 class classifier mainly of Yoda, Luke and Wookie. The model development will be done using fastai. If you like the notebook, consider giving an upvote. ✅ Table of Contents: . Downloads | Packages | Pre-model building 3.1 Create folder | 3.2 Scrape images | 3.3 Move images | . | Data Loaders 4.1 For a single label | 4.2 For the model building | . | Model Building 5.1 Training | 5.2 Prediction | . | 1. Downloads . back to top . I&#39;m using a python package named icrawler for scraping the images. . !pip install icrawler . Collecting icrawler Downloading icrawler-0.6.4.tar.gz (26 kB) Collecting beautifulsoup4&gt;=4.4.1 Using cached beautifulsoup4-4.9.3-py3-none-any.whl (115 kB) Collecting lxml Using cached lxml-4.6.3-cp38-cp38-macosx_10_9_x86_64.whl (4.6 MB) Collecting requests&gt;=2.9.1 Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 5.1 MB/s eta 0:00:01 Requirement already satisfied: six&gt;=1.10.0 in /Users/namanmanchanda/miniconda3/envs/nishu/lib/python3.8/site-packages (from icrawler) (1.15.0) Requirement already satisfied: Pillow in /Users/namanmanchanda/miniconda3/envs/nishu/lib/python3.8/site-packages (from icrawler) (8.1.1) Collecting soupsieve&gt;1.2 Using cached soupsieve-2.2.1-py3-none-any.whl (33 kB) Collecting idna&lt;3,&gt;=2.5 Downloading idna-2.10-py2.py3-none-any.whl (58 kB) |████████████████████████████████| 58 kB 9.3 MB/s eta 0:00:01 Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/namanmanchanda/miniconda3/envs/nishu/lib/python3.8/site-packages (from requests&gt;=2.9.1-&gt;icrawler) (2020.12.5) Collecting chardet&lt;5,&gt;=3.0.2 Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB) |████████████████████████████████| 178 kB 19.4 MB/s eta 0:00:01 Collecting urllib3&lt;1.27,&gt;=1.21.1 Downloading urllib3-1.26.5-py2.py3-none-any.whl (138 kB) |████████████████████████████████| 138 kB 9.4 MB/s eta 0:00:01 Building wheels for collected packages: icrawler Building wheel for icrawler (setup.py) ... done Created wheel for icrawler: filename=icrawler-0.6.4-py2.py3-none-any.whl size=35063 sha256=bd50c3f1d07da534fa4b7615bd47053e2d92da940734f7757b3c9d172fc18b9d Stored in directory: /Users/namanmanchanda/Library/Caches/pip/wheels/5b/a5/50/db28e1726fdc127cb6c5a757a4350af44a32b4e5d2c5d45dac Successfully built icrawler Installing collected packages: urllib3, soupsieve, idna, chardet, requests, lxml, beautifulsoup4, icrawler Successfully installed beautifulsoup4-4.9.3 chardet-4.0.0 icrawler-0.6.4 idna-2.10 lxml-4.6.3 requests-2.25.1 soupsieve-2.2.1 urllib3-1.26.5 . . 2. Packages . back to top . import os import numpy as np import pandas as pd import matplotlib.pyplot as plt for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # icrawler from icrawler.builtin import GoogleImageCrawler # fastai from fastai import * from fastai.vision import * from fastai.imports import * from fastai.vision.all import * # widgets import ipywidgets as widgets # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) . 3. Pre-model building . back to top . 3.1 Create folder . I&#39;m creating three folders in /kaggle/working to download their respective image in each folder. . !mkdir yoda !mkdir luke !mkdir wookie . 3.2 Scrape images . The way icrawler works is that it creates a folder named /images from wherever the command is run. So right now we are in the /kaggle/working directory. Now I&#39;ll be going to each directory one at a time and run the crawler to download the images in the /images folder. So the structure of the folders will be something like . /kaggle/working/yoda/images | /kaggle/working/luke/images | /kaggle/working/wookie/images | . After the download, I&#39;ll be moving the images from the images folder of each respective label to the label folder itself - so for example from /kaggle/working/yoda/images to /kaggle/working/yoda and I&#39;ll be deleting all the empty images folder. . If you would like to reproduce the exact same thing, run the command in console first followed by the command in the notebook and so on in the provided order which is as follows. . Run the following in console . cd yoda After above command, run the following cell . google_crawler = GoogleImageCrawler() google_crawler.crawl(keyword=&#39;baby yoda&#39;, max_num=50) . Run the following in console one line at a time . cd .. cd luke After above command, run the following cell . google_crawler = GoogleImageCrawler() google_crawler.crawl(keyword=&#39;luke skywalker&#39;, max_num=50) . Run the following in console one line at a time . cd .. cd wookie After above command, run the following cell . google_crawler = GoogleImageCrawler() google_crawler.crawl(keyword=&#39;wookie&#39;, max_num=50) . 3.3 Move images . Run the following in console one line at a time . cd .. Now I&#39;ll be moving the images from /images folders to their respective labels and deleting the /images folder. Run the following in console one line at a time . mv -v yoda/images/* yoda mv -v luke/images/* luke mv -v wookie/images/* wookie rmdir yoda/images rmdir luke/images rmdir wookie/images Once done, you may check run pwd in console to check your current working directory. It must show /kaggle/working. . 4. Data Loaders . back to top . 4.1 For a single label . path = Path(&#39;/kaggle/working/yoda&#39;) dls = ImageDataLoaders.from_folder(path, valid_pct=0.5, batch_size=10, item_tfms=Resize(224)) dls.valid.show_batch(max_n=4, nrows=1) . 4.2 For the model building . characters = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(256)) # Creating the dataloader path = Path(&#39;/kaggle/working&#39;) dls = characters.dataloaders(path) # checking the images dls.valid.show_batch(max_n=18, nrows=3) . 5. Model Building . back to top . 5.1 Training . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . epoch train_loss valid_loss error_rate time . 0 | 1.834176 | 2.258008 | 0.600000 | 00:07 | . epoch train_loss valid_loss error_rate time . 0 | 1.978312 | 1.446693 | 0.566667 | 00:06 | . 1 | 1.585535 | 0.461595 | 0.133333 | 00:05 | . 2 | 1.268723 | 0.311160 | 0.066667 | 00:05 | . 3 | 1.044068 | 0.287985 | 0.066667 | 00:05 | . 5.2 Prediction . uploader = widgets.FileUpload() uploader . def helper(): img = PILImage.create(uploader.data[0]) img.show() pred,pred_idx,probs = learn.predict(img) lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; print(lbl_pred) . helper() . Label(value=&#39;Prediction: yoda; Probability: 0.9999&#39;) . helper() . Label(value=&#39;Prediction: luke; Probability: 0.9567&#39;) . helper() . Label(value=&#39;Prediction: wookie; Probability: 0.9970&#39;) . If you liked the notebook, please drop a upvote. Thank you.✅ Check out my other notebooks here: . https://www.kaggle.com/namanmanchanda/cat-vs-dog-classifier-10-lines-of-code-fast-ai | https://www.kaggle.com/namanmanchanda/titanic-eda | https://www.kaggle.com/namanmanchanda/pima-indian-diabetes-eda-and-prediction |",
            "url": "https://namanmanchanda09.github.io/mindAI/fastpages/jupyter/2021/05/27/starwars.html",
            "relUrl": "/fastpages/jupyter/2021/05/27/starwars.html",
            "date": " • May 27, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Naman and I’m in my senior year of study pursuing engineering from GGSIPU. What drives me is DATA. I love to play with Data and the most fun part, the story it tells. . I believe in continous learning and so, keep exploring every technical field. I have explored every field from Blockchain to Deep Learning. I have created a few decentralised applications on Ethereum, full stack apps and done data analysis. I have created some really cool projects using Computer Vision technology some of which include American Sign Language Detection etc. Check out my links to know more about me. . Blog | Twitter | Github | Linkedin | Kaggle | Anything urgent, text me on Discord - Naman Manchanda#8016 🙂 .",
          "url": "https://namanmanchanda09.github.io/mindAI/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://namanmanchanda09.github.io/mindAI/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}